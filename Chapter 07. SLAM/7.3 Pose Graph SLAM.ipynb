{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c5ede4",
   "metadata": {},
   "source": [
    "# 7.3 Least Squares Optimization and Pose Graph SLAM \n",
    "\n",
    "In the last two chapters, we have looked at two of the most fundamental problems in probabilistic mobile robotics: ***localization*** and ***mapping***. In the localization chapter we saw how, knowing the map of the environment that the robot is moving through, it is possible to use sensory readings to correct errors in the odometry and figure out the pose of the robot. In the mapping chapter, we learnt how to integrate the sensory measurements taken from multiple different poses to *build* a map, using the probabilistic nature of the observation model to account for the uncertainty in the measurements.\n",
    "\n",
    "However, this is a bit of a chicken-and-egg problem: if we need to know the map before we can localize ourselves, but we need to know our pose to build the map, how can we ever do anything? The solution, it turns out, is to do both things at once: **Simultaneous Localization And Mapping (SLAM)**.\n",
    "\n",
    "<figure  style=\"text-align:center;\">\n",
    "    <video width=\"720\" controls>\n",
    "    <source src=\"images/posegraph/slam.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <figcaption>If we try to use the sensor readings to build a map, but do not know the pose the reading was taken from, the resulting map is going to be quite bad.</figcaption>\n",
    "</figure>\n",
    "\n",
    "As you can imagine, this is a fairly complex problem in which the uncertainty can easily get out of hand, as we are constantly basing imperfect observations on top of imperfect pose estimations. Luckily for us, very smart people have come up with multiple approaches to tackle this problem, including variations of techniques we are already familiar with, such as [particle filters](https://openslam-org.github.io/gmapping.html) and the [EKF](http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/slam04-ekf-slam.pdf). In this chapter, however, we are going to focus on a new technique: the **Pose Graph**.\n",
    "\n",
    "The reasoning behind most SLAM methods (including Pose Graphs) is as follows: in order to build an occupancy map from laser scanner readings, we need to know where each reading was taken from. Thus, if we can find the sequence of poses that the robot has moved through, we can then use those poses to project the laser pointclouds into world space, and build the map with the same method we saw in the *occupancy grid mapping* notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0f969-560a-4aab-b80a-51aea730597b",
   "metadata": {},
   "source": [
    "## 7.3.1 Pose Graphs\n",
    "\n",
    "At its core, the idea of Pose Graphs is quite simple: we are going to represent the trajectory of the robot with a **graph**, where:\n",
    "- each **node** represents a pose, and\n",
    "- the **arcs** between the nodes (which are sometimes referred to as *factors*) represent the expected relative transformation between the poses, based on whatever sensory information we are able to use. \n",
    "\n",
    "For example, if we only have odometry, we might end up with something like this:\n",
    "\n",
    "<br /><center><figure style=\"text-align:center; width:700px\">\n",
    "  <img src=\"images/posegraph/graph-odom.png\" alt=\"\">\n",
    "  <figcaption>Node graph with odometry information only.</figcaption>\n",
    "</figure></center><br />\n",
    "\n",
    "Let's assume we want to define the starting pose as $pose_1 = [0, 0, 0] ^T$. This graph tells us that, according to our data, \n",
    "$\\\\[2pt]$ $$pose_2 = pose_1 \\oplus u_1$$ and $$pose_3 =  pose_2 \\oplus u_2$$ $\\\\[2pt]$ \n",
    "The idea, then, is to find the values for each of the poses that satisfy this system of equations. However, you can probably see that this is not going to be enough to solve our problem, since the solution to those equations is just the odometry-based trajectory. As we know, the odometry accumulates error over time (*drift*), and this formulation does not allow us to correct that error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d9846-e7f7-4ae7-8ac7-c75fe062a8f5",
   "metadata": {},
   "source": [
    "### Loop closures\n",
    "\n",
    "In order to make this helpful, then, we need to incorporate more than just the odometry. We need to use some other source of sensory information to create more arcs between the nodes -- or, put another way, *add more equations* to the system. This is called a **loop closure**: an observation that lets us relate a new pose to some pre-existing node in the graph (creating a loop):\n",
    "\n",
    "<br><center><figure style=\"text-align:center; width:700px\">\n",
    "  <img src=\"images/posegraph/graph-loop-closure.png\" alt=\"\">\n",
    "<figcaption>Node graph with a loop closure. The loop closure is based on finding a match between the observations taken from poses 1 and 3. This is produced, for example, by <b>a common observed landmark</b> or by <b>scan registration</b>.</figcaption> \n",
    "</figure></center><br>\n",
    "\n",
    "Now, our system of equations looks like this: \n",
    "\n",
    "\\begin{split}\n",
    "pose_1 &= [0,0,0]^T \\\\\n",
    "pose_2 &= pose_1 \\oplus u_1 \\\\\n",
    "pose_3 &= pose_2 \\oplus u_2 \\\\\n",
    "pose_1 &= pose_3 \\oplus p_z^{31}\n",
    "\\end{split}\n",
    "\n",
    "And now we have an interesting situation. If our odometry and sensor measurements are unreliable (which they are), we can in general expect that: $\\\\[2pt]$\n",
    "\n",
    "$$\n",
    "pose_1 \\oplus u_1 \\oplus u_2 \\oplus p_z^{31} \\neq pose_1\n",
    "$$ \n",
    "\n",
    "In other words, the loop does not match! This, of course, is precisely what we want to correct.\n",
    "\n",
    "In order to tackle this, let's consider our situation. Remember that, in the previous case (pure odometry), we had a system of equations with three equations, and three poses to estimate. Now, we have four equations, but still three poses: we have an **overdetermined system**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe5a29-ca9b-4a38-951e-786f383d7b89",
   "metadata": {},
   "source": [
    "## 7.3.2 Overdetermined Systems and Least Squares\n",
    "\n",
    "In general, when dealing with a problem that involves an overdetermined system, it is not possible to find a perfect solution, as equations that are derived from observations are subject to sensor noise. Let's think about the simplest possible example, where we have a single variable. Let's say that we want to figure out the distance from the robot to a wall (let's call this distance $d$), and we have a range sensor that will provide us with a distance measurement. Due to sensor noise, we might find that after taking three measurements, our equations look like this:\n",
    "\n",
    "\\begin{split}\n",
    "d &= 1.00 \\\\ \n",
    "d &= 0.95 \\\\\n",
    "d &= 1.05 \\\\\n",
    "\\end{split}\n",
    "\n",
    "Evidently, there is no value of $d$ that will satisfy all three equations. However, it seems intuitive that there is a solution $\\hat d$ which *best matches* the set of observations, by making the **sum of all the errors** be as small as possible. In practice, it is better to minimize the **sum of the *squared* errors** instead, as this has multiple advantages (which are beyond the scope of this discussion). For example, in this case, if $\\hat d=1.0$ then \n",
    "\n",
    "  * $e_1=(\\hat d - d_1)^2=0$,\n",
    "\n",
    "  * $e_2=(\\hat d - d_2)^2=0.0025$ and \n",
    "\n",
    "  * $e_3=(\\hat d - d_3)^2=0.0025$, \n",
    "\n",
    "so $e_1+e_2+e_3 = 0.005$, \n",
    "\n",
    "and there is no other value of $\\hat d$ that provides a lower total squared error. It makes sense, then, to consider $\\hat d=1.0$ the best guess for the correct value of $d$, even though some error still exists.\n",
    "\n",
    "This is a **very** widely used technique, called **Least Squares Optimization**. Let's explore this technique a little bit, looking at some simple examples, before we return to the SLAM problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593f289",
   "metadata": {},
   "source": [
    "### Linear Least Squares\n",
    "\n",
    "A linear system of equations can be represented with a matrix $H$, which contains the coefficients of each of the variables in the equations. If we are trying to estimate a vector $x$ based on observations $z$, this would be expressed as \n",
    "\n",
    "$$\n",
    "z = H x\n",
    "$$\n",
    "\n",
    "Here, $H$ codifies our observation model. For example:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left\\{\n",
    "\\begin{split}\n",
    "c &= a \\cdot x_1 + b \\cdot x_2  \\\\\n",
    "f &= d \\cdot x_1 + e \\cdot x_2  \\\\\n",
    "\\end{split} \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix} c \\\\ f \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    a & b \\\\ d & e \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Generally, we want the same number of equations ($n$) as variables ($m$). Remember that each observation $z$ defines an equation. In such a case, where $n=m$, a simple solution to this problem exists:\n",
    "$$x = H ^{-1} z$$\n",
    "\n",
    "So a unique solution exists if $H$ is invertible; that is, if $H$ is a square matrix with $det(H) \\neq 0$.\n",
    "\n",
    "However, when dealing with an overdetermined system, $H$ is not a square matrix, and thus not invertible. In such cases, we can obtain the **least squares** solution by using the *pseudo-inverse* of the matrix $H$:\n",
    "\n",
    "$$ \\hat x = \\arg\\min_x e^T e  = \\arg\\min_x [(z-Hx)^T(z-Hx)] = \\arg\\min_x || z-Hx ||^2$$\n",
    "\n",
    "$$\\hat{x} = \\underbrace{(H^T H)^{-1} H^T}_{\\textit{pseudo-inverse }(H^+)} z$$\n",
    "\n",
    "Curious about how is that expression reached?\n",
    "\n",
    "To find the estimate $\\hat{x}$ that minimizes $|| z-Hx ||^2$, we take its derivative with respect to $x$ and set it to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial || z-Hx ||^2}{\\partial x} = -2H^T(z-Hx) = 0\n",
    "$$\n",
    "\n",
    "Rearranging this equation we get:\n",
    "\n",
    "$$\n",
    "H^THx = H^T z\n",
    "$$\n",
    "\n",
    "And finally, assuming $H^TH$ invertible, we can solve for $\\hat{x}$ as:\n",
    "\n",
    "$$\\hat{x} = (H^T H)^{-1} H^T z$$\n",
    "\n",
    "This is the **Least Squares solution** for the state vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ae0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize, stats, spatial\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.DrawRobot import DrawRobot\n",
    "from utils.tcomp import tcomp, inv_tcomp\n",
    "from utils.laser.laser2D import Laser2D\n",
    "from utils.unit7.occupancy_mapping import OccupancyGridMap, process_new_observation, plot_grid_map\n",
    "from laserUtils import LaserUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a2aac",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Playing with a robot in a corridor</i></b></span>** \n",
    "\n",
    "The following code illustrates a simple scenario where a robot is in a corridor looking at a door, which is placed at the origin of the reference system (see Fig.1). The robot is equipped with a laser scanner able to measure distances, and takes a number of observations $z$. The robot is placed 3 meters away from the door, but this information is unknown for it. **Your goal is** to estimate the position of the robot in this 1D world using those measurements. $\\\\[5pt]$\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/corridor.png\" alt=\"\">\n",
    "  <figcaption>Fig. 1: Simple 1D scenario with a robot equipped with a laser scanner measuring distance to a door.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The following code cell shows the dimensions of all the actors involved in LS-positioning. Complete it for computing the robot pose $x$ from the available information. *Recall [`np.linalg.inv()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) to obtain the inverse of a matrix.* \n",
    "\n",
    "*Note that with this configuration the laser is directly measuring the robot position. If the laser was measuring distances an object placed at a different location $m_i$, then the equation should be $z_i=Hx-m_i$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the robot pose to unknown\n",
    "x = np.vstack(np.array([None]))\n",
    "\n",
    "# Sensor measurements to the door\n",
    "z = np.vstack(np.array([3.7,2.9,3.6,2.5,3.5]))\n",
    "\n",
    "# Observation model\n",
    "H = np.ones(np.array([5,1]))\n",
    "\n",
    "print (\"Dimensions:\")\n",
    "print (\"Pose x:         \" + str(x.shape))\n",
    "print (\"Observations z: \" + str(z.shape))\n",
    "print (\"Obs. model H:   \" + str(H.shape))\n",
    "print (\"H.T@H:          \" + str((H.T@H).shape))\n",
    "print (\"inv(H.T@H):     \" + str((np.linalg.inv(H.T@H)).shape))\n",
    "print (\"H.T@z :         \" + str((H.T@z).shape))\n",
    "\n",
    "# Do Least Squares Positioning\n",
    "x = None\n",
    "\n",
    "print('\\nLS-Positioning')\n",
    "print('x = ' + str(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b27d6",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output</span>\n",
    "\n",
    "```\n",
    "x = [3.24]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00dd9a",
   "metadata": {},
   "source": [
    "## 7.3.3 Weighted measurements\n",
    "\n",
    "In cases where multiple sensors affected by different noise profiles are used, or in those where the robot is using a sensor with a varying error (*e.g.* typically radial laser scans are more accurate while measuring distances to close objects), it is interesting to weight the contribution of such measurements while retrieving the robot pose. For example, we are going to consider a sensor whose accuracy drops the further the observed landmark is. Given a *covariance* matrix $Q$ describing the error in the measurements, the equations above are rewritten as:\n",
    "\n",
    "  $$ \\hat x = \\arg\\min_{x} e^T Q^{-1} e = [(Hx-z)^TQ^{-1}(Hx-z)] $$\n",
    "\n",
    "  $$\n",
    "    \\begin{aligned}\n",
    "      &\\hat{x} \\leftarrow (H^T Q^{-1} H)^{-1} H^T Q^{-1} z &\\text{(1. Best estimation)}\\\\ \n",
    "      &\\Sigma_{\\hat{x}} \\leftarrow (H^T Q^{-1} H)^{-1} &\\text{(2. Uncertainty of the estimation)}\\\\\n",
    "    \\end{aligned}\n",
    "  $$\n",
    "  \n",
    "  Example with three measurements having different uncertainty ($\\sigma_1^2$, $\\sigma_2^2$, $\\sigma_3^2$):\n",
    "  \n",
    "  $$\n",
    "  e^T Q^{-1} e = [e_1 \\; e_2 \\; e_3]\n",
    "  \\begin{bmatrix} 1 / \\sigma_1^2 & 0 & 0 \\\\ 0 & 1/\\sigma_2^2 & 0 \\\\ 0 & 0 & 1/\\sigma_3^2 \\end{bmatrix} \n",
    "  \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\end{bmatrix}\n",
    "  = \\frac{e_1^2}{\\sigma_1^2} + \\frac{e_2^2}{\\sigma_2^2} + \\frac{e_3^2}{\\sigma_3^2}\n",
    "  = \\sum_{i=1}^m \\frac{e_i^2}{\\sigma_i^2}\n",
    "  $$\n",
    "  \n",
    "  In this way, the bigger the $\\sigma_i^2$, the smaller its contribution to the pose's computation.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c67d2",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Adding growing uncertainty</i></b></span>** \n",
    "\n",
    "We have new information! The manufacturer of the laser scanner mounted on the robot sent us an email, telling us that the device is considerably less accurate for large distances. Concretely, such uncertainty is characterized by $\\sigma^2=e^z$ (the laser is not great, being polite).\n",
    "\n",
    "With this new information, implement the computation of the weighted LS-positioning so you can compare the previously estimated position with the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor measurements to the door\n",
    "z = np.vstack(np.array([3.7,2.9,3.6,2.5,3.5]))\n",
    "\n",
    "# Uncertainty of the measurements\n",
    "Q = np.eye(5)*np.exp(z)\n",
    "\n",
    "# Observation model\n",
    "H = np.ones(np.array([5,1]))\n",
    "\n",
    "# Do Least Squares Positioning\n",
    "x = None\n",
    "\n",
    "# Do Weighted Least Squares Positioning\n",
    "x_w = None\n",
    "\n",
    "print('\\nLS-Positioning')\n",
    "print('x = ' + str(x[0]))\n",
    "\n",
    "print('\\nWeighted-LS-Positioning')\n",
    "print('x = ' + str(np.round(x_w[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e053ad7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output</span>\n",
    "\n",
    "```\n",
    "LS-Positioning\n",
    "x = [3.24]\n",
    "\n",
    "Weighted-LS-Positioning\n",
    "x = [3.01]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e09c47",
   "metadata": {},
   "source": [
    "## 7.3.4 Non-linear Least Squares\n",
    "\n",
    "So far, we have seen how to find the least squares solution for linear systems of equations, which can be represented with a matrix $H$. However, our pose graph is built from pose composition, which is a non-linear operation.\n",
    "\n",
    "Sadly, there is no closed-form solution for non-linear least squares. Still, we can try to find a solution through iterative optimization. There are many optimization methods that will take in an initial guess for the solution, and keep updating that guess until they find a minimum in the error function:\n",
    "\n",
    "$$ \\hat x = \\arg\\min_x || z-h(x) ||^2$$\n",
    "\n",
    "Despite the fact that many approaches exist, they all follow the same structure:\n",
    "\n",
    "```python\n",
    "# generic non-linear solver\n",
    "def optimize(initial_guess, convergence_threshold):\n",
    "    guess = initial_guess\n",
    "    while(error_function(guess) > convergence_threshold):\n",
    "        # most methods use the first and second derivatives of the error function\n",
    "        # to calculate the best increment\n",
    "        increment = calculate_best_increment(guess) \n",
    "        guess += increment\n",
    "    return guess\n",
    "```\n",
    "\n",
    "This means that, so long as we can tell the solver how to compute the `error(guess)` and (for some methods) its derivatives, the solver can deal with any optimization problem! This abstraction allows us to focus on defining *the problem*, which we know and understand, rather than the numerical methods to find the solution. Neat!\n",
    "\n",
    "However, there are also some bad news. In general, there is no guarantee that the solution $\\hat x$ that these methods produce is the global minimum. Iterative methods are always vulnerable to converging on local minima when applied to non-convex functions, although there [some techniques](https://en.wikipedia.org/wiki/Global_optimization) (which we will not worry about today) to *try* and escape them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c877f0ea",
   "metadata": {},
   "source": [
    "### A brief pause to collect our thoughts\n",
    "\n",
    "Ok, so to sum up what we have talked about so far:\n",
    "\n",
    "- A **pose graph** defines the relative transforms that we expect between the poses in our trajectory. These relative poses will initially be derived from the odometry. We can also express the graph as a system of equations, using these relative transformations as the equations, and the poses as the unknowns.\n",
    "- **Loop closures** in the graph allow us to include additional constraints to our system of equations, which will make our system overdetermined and allow us to correct the odometry errors.\n",
    "- **Least squares** is the standard method to find a solution for overdetermined systems. It will not perfectly match all the equations (as that is not possible), but will minimize the error. \n",
    "- Linear least squares can be solved directly in matrix form, but non-linear problems require iterative methods (like Gauss-Newton).\n",
    "- Non-linear LS solvers are problem-agnostic algorithms: you do not have to implement them from scratch. You only need to tell the solver how to compute the error for a given solution and (possibly) its derivatives.\n",
    "\n",
    "Ok, then. It seems what we have to do is define our problem (pose graph optimization) in such a way that we can **calculate the error of a given solution**, and then use a **non-linear least-squares solver** to minimize that error. Let's get on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b49ec",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "At this point you are able to **address the following points**:\n",
    "\n",
    "- We talk in the previous sections about *linear* and *non-linear* systems. What do we mean by this? What determines whether we are dealing with a linear or non-linear problem?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>  \n",
    "\n",
    "- How is this *error minimization* relevant to the problem of SLAM? What are the errors that we are going to minimize?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>  \n",
    "\n",
    "- What makes the problem of pose graph optimization non-linear? \n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3af10b",
   "metadata": {},
   "source": [
    "## 7.3.5 Defining the pose graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4e60f",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Nodes and Factors</i></b></span>** \n",
    "\n",
    "Let's start simple. You are provided with a class `Node` which represents each of the poses that go into the graph. On top of the pose, nodes also have a boolean field `is_fixed`, which controls whether the solver is allowed to move them. This is mainly there because we want to make sure the starting pose remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f26a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, pose : np.ndarray, fixed: bool = False):\n",
    "        self.pose = np.array(pose)\n",
    "        self.is_fixed = fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102cd13",
   "metadata": {},
   "source": [
    "You are tasked with implementing a class `Factor`, which represents the relation between two nodes (an arc in the graph). Remember that each of these arcs codifies the expected relative transformation between the two poses.\n",
    "\n",
    "This class will store the following information:\n",
    "\n",
    "- The two nodes ($node_1$ and $node_2$) in the graph which it links together. These will just be stored as indices into a list, which is kept elsewhere.\n",
    "\n",
    "- The expected transformation between the pose of $node_1$ ($pose_1$) and the pose of $node_2$ ($pose_2$), which will be provided by odometry or loop closure detection (a common observed landmark or scan registrantion providing a relative transformation). It's important to note that this transformation is expressed in the system of reference of $pose_1$:\n",
    "$$\n",
    "expected = pose_1 \\ominus pose_2\n",
    "$$\n",
    "\n",
    "- The weight associated with this factor. As we saw in the linear case before, this weight is just the inverse of the covariance matrix of the relative pose: $W = Q^{-1}$. \n",
    "\n",
    "\n",
    "The `Factor` class also provides a method, `evaluate_error()`, which computes how well a solution (specific values for $pose_1$ and $pose_2$) matches its expected transformation.\n",
    "\n",
    "$$\n",
    "error = e^TQ^{-1}e,\n",
    "$$\n",
    "with\n",
    "\n",
    "$$\n",
    "e = pose_2 - (pose_1 \\oplus expected)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factor:\n",
    "    def __init__(self, node1_ind, node2_ind, relative_pose, relative_pose_covariance):\n",
    "        self.node1_ind = node1_ind\n",
    "        self.node2_ind = node2_ind\n",
    "        self.expected_transform = relative_pose\n",
    "        self.weight = None\n",
    "    \n",
    "    def evaluate_error(self, pose1, pose2):\n",
    "        expected_p2 = None\n",
    "        difference = None\n",
    "        \n",
    "        # always take the shortest difference between the angles\n",
    "        ang = difference[2] % (2 * np.pi)\n",
    "        ang = ang if ang<np.pi else -(2*np.pi -ang) \n",
    "        difference[2] = ang\n",
    "\n",
    "        v = None\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d25b2",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Connecting the nodes into a graph</i></b></span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f435717",
   "metadata": {},
   "source": [
    "Now that we have both the nodes and the factors that connect them to each other, we can build a graph. The provided class `PoseGraph` will be used to hold all the data related to the nodes and factors.\n",
    "\n",
    "This class provides some methods to handle visualization, which you don't need to worry about. It also handles a bit of inconvenient translation work: least-squares solvers expect the parameters (the elements of the solution) to be a single, 1D list of numbers, but we have a list of *poses* that we want the solver to modify. So, the methods `solver_params_from_nodes()` and `set_nodes_from_solution()` are provided to flatten and un-flatten the poses as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6917ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseGraph:\n",
    "    def __init__(self):\n",
    "        self.nodes : list[Node] = []\n",
    "        self.factors : list[Factor] = []\n",
    "        self.markers = [] # for visualization\n",
    "\n",
    "    # get a single array with all the flattened poses concatenated (for the optimizer)\n",
    "    def solver_params_from_nodes(self):\n",
    "        params = np.zeros(3 * len(self.nodes))\n",
    "        \n",
    "        # the \"bounds\" arrays are used to prevent  the solver from touching fixed nodes\n",
    "        upper_bound = np.full(params.shape, np.inf)\n",
    "        lower_bound = np.full(params.shape, -np.inf)\n",
    "        \n",
    "        i = 0\n",
    "        for node in self.nodes:\n",
    "            params[i] = node.pose[0,0]\n",
    "            params[i+1] = node.pose[1,0]\n",
    "            params[i+2] = node.pose[2,0]\n",
    "\n",
    "            if(node.is_fixed):\n",
    "                upper_bound[i] = node.pose[0,0]\n",
    "                upper_bound[i+1] = node.pose[1,0]\n",
    "                upper_bound[i+2] = node.pose[2,0]\n",
    "\n",
    "                lower_bound[i] = upper_bound[i] \n",
    "                lower_bound[i+1] = upper_bound[i+1]\n",
    "                lower_bound[i+2] = upper_bound[i+2]\n",
    "\n",
    "            i+=3\n",
    "\n",
    "        bounds = optimize.Bounds(ub=upper_bound, lb=lower_bound, keep_feasible=True)\n",
    "        return params, bounds\n",
    "    \n",
    "    def set_nodes_from_solution(self, solution):\n",
    "        i = 0\n",
    "        for node in self.nodes:\n",
    "            node.pose[0,0] = solution[i]\n",
    "            node.pose[1,0] = solution[i+1]\n",
    "            node.pose[2,0] = solution[i+2]\n",
    "            i+=3\n",
    "\n",
    "    def set_output(self, fig, ax):\n",
    "        if not hasattr(self, 'intermediate_stdout'):\n",
    "            self.intermediate_stdout = widgets.Output()\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "        \n",
    "    def visualize(self):\n",
    "        self.__remove_markers()\n",
    "        for node in self.nodes:\n",
    "            marker = DrawRobot(self.fig, self.ax, node.pose, color=\"green\")\n",
    "            self.markers.extend(marker)\n",
    "        self.show()\n",
    "\n",
    "    def draw_intermediates(self, intermediate_result):\n",
    "        # print some numbers. Some solvers just return the current solution without any stats, so we might have to skip this\n",
    "        if type(intermediate_result) is optimize.OptimizeResult:\n",
    "            with self.intermediate_stdout:\n",
    "                print(f\"\\rResidual: {intermediate_result.fun}                                      \", end='')\n",
    "        \n",
    "        # plot the node poses in the map\n",
    "        flattened_poses = intermediate_result.x if type(intermediate_result) is optimize.OptimizeResult else intermediate_result\n",
    "        i= 0\n",
    "        while i < len(flattened_poses):\n",
    "            pose = flattened_poses[i:i+3]\n",
    "            marker = DrawRobot(self.fig, self.ax, np.vstack(pose), color=\"green\")\n",
    "            self.markers.extend(marker)\n",
    "            i+=3\n",
    "\n",
    "        # show to the screen and remove the markers from the figure for next iteration\n",
    "        self.show()\n",
    "        self.__remove_markers()\n",
    "\n",
    "    def __remove_markers(self):\n",
    "        for marker in self.markers:\n",
    "            marker.remove()\n",
    "        self.markers = []\n",
    "\n",
    "    def show(self):\n",
    "        if not hasattr(self, 'display_handle'):\n",
    "            self.display_handle = display(self.fig, display_id=True)\n",
    "            self.intermediate_stdout_handle = display(self.intermediate_stdout, display_id=True)\n",
    "        else:\n",
    "            self.display_handle.update(self.fig)\n",
    "            self.intermediate_stdout_handle.update(self.intermediate_stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c25643",
   "metadata": {},
   "source": [
    "Let's populate this graph! **You will need to implement two functions**:\n",
    "\n",
    "- `add_odom_factor()` will receive the estimated current robot pose (pure odometry), and the relative transformation that was used to compute it from the previous pose, along with the covariance of this relative transform. This function must create a new node to represent this new pose and add it to the graph. Then, it must connect it to the previous most-recent node through a factor which represents this odometry-based relative transformation.\n",
    "\n",
    "- `add_loop_closure()` receives two arbitrary pose indices (which refer to the `graph.nodes` list), and creates a new factor that connects them. The arguments of the function also include the relative transform and its uncertainty which are associated to this factor.\n",
    "\n",
    "In this notebook, we are not going to worry about how the loop closures are detected. Depending on the sensors that the robot has access to, this process could be done by **re-observing landmarks** (visual sensors) or by running point-cloud registration algorithms like **ICP** (laser scanners). The key takeaway is that, once the loop closure has been detected, the solver does not care about how that detection happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c89e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_odom_factor(graph: PoseGraph, new_pose, odometry_transform, odom_covariance):\n",
    "    node_count = len(graph.nodes)\n",
    "    most_recent_node_ind = node_count-1\n",
    "    graph.nodes.append(None)\n",
    "    graph.factors.append(None)\n",
    "\n",
    "def add_loop_closure(graph: PoseGraph, pose1_ind, pose2_ind, relative_transform, loop_covariance):\n",
    "    graph.factors.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cddab",
   "metadata": {},
   "source": [
    "You can use the following code to make sure the factors have been defined corretly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77734e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_data(graph: PoseGraph):\n",
    "    for factor in graph.factors:\n",
    "        print(f\"Pose {factor.node1_ind} -> Pose {factor.node2_ind}:\\n{factor.expected_transform}\")\n",
    "\n",
    "def test_graph():\n",
    "    graph=PoseGraph()\n",
    "    pose = np.vstack([0,0,0])\n",
    "    graph.nodes.append(Node(pose, fixed=True))\n",
    "    \n",
    "    movement = np.vstack([0.1, 0., 0.2])\n",
    "    movement_cov = np.diag([0.1, 0.1, 0.1])\n",
    "    for i in range(4):\n",
    "        add_odom_factor(graph, pose, movement, movement_cov)\n",
    "\n",
    "    add_loop_closure(graph, \n",
    "                     pose1_ind=4, pose2_ind=2, \n",
    "                     relative_transform=np.vstack([3,3,0]), \n",
    "                     loop_covariance=np.diag([0.01, 0.01, 0.01]))\n",
    "    print_graph_data(graph)\n",
    "\n",
    "test_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62589524",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Pose 0 -> Pose 1:\n",
    "[[0.1]\n",
    " [0. ]\n",
    " [0.2]]\n",
    "Pose 1 -> Pose 2:\n",
    "[[0.1]\n",
    " [0. ]\n",
    " [0.2]]\n",
    "Pose 2 -> Pose 3:\n",
    "[[0.1]\n",
    " [0. ]\n",
    " [0.2]]\n",
    "Pose 3 -> Pose 4:\n",
    "[[0.1]\n",
    " [0. ]\n",
    " [0.2]]\n",
    "Pose 4 -> Pose 2:\n",
    "[[3]\n",
    " [3]\n",
    " [0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c8336",
   "metadata": {},
   "source": [
    "### Getting the total error and calling the solver \n",
    "\n",
    "With our `PoseGraph` defined, we can now run the solver to find the optimal sequence of poses that \"satisfies\" both the odometry and the loop closures. The provided functions `evaluate_total_error()` and `run_solver()` do just that: define the error function that the solver should minimize (the sum of the weighted squared errors)\n",
    "\n",
    "$$\n",
    "e = \\sum_i e_i^T Q_i^{-1} e_i\n",
    "$$\n",
    "\n",
    ", and configure the solver to run the optimization process with this error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_total_error(graph: PoseGraph, flattened_poses):\n",
    "    total = 0.\n",
    "    for factor in graph.factors:\n",
    "        pose1 = np.vstack(flattened_poses[factor.node1_ind*3 : factor.node1_ind*3+3])\n",
    "        pose2 = np.vstack(flattened_poses[factor.node2_ind*3 : factor.node2_ind*3+3])\n",
    "        total += None\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48507859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_solver(graph: PoseGraph, method = 'Powell'):\n",
    "    parameters, bounds = graph.solver_params_from_nodes()\n",
    "    error_func = functools.partial(evaluate_total_error, graph)\n",
    "\n",
    "    # Run the solver!\n",
    "    result : optimize.OptimizeResult = optimize.minimize(\n",
    "            error_func, # error function (what we are trying to minimize!) This is not a *value*, but a *callable function*. The solver will repeatedly call it to evaluate many possible solutions\n",
    "            x0 = parameters, # initial guess\n",
    "            bounds=bounds, # this is used to specify limits to the value of the parameters. In our case, to keep the starting pose static\n",
    "            method=method, \n",
    "            tol=1e-8, \n",
    "            options={\n",
    "                \"maxiter\" : 10000,\n",
    "                \"ftol\" : 1e-2\n",
    "            },\n",
    "            callback=graph.draw_intermediates\n",
    "            )\n",
    "    print(f\"Finished optimization with status : {result.status} after {result.nit} iterations. \\nResidual: {result.fun}\\nMessage: '{result.message}'\")\n",
    "    \n",
    "    # apply the solution we found to our list of nodes\n",
    "    graph.set_nodes_from_solution(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aae6be",
   "metadata": {},
   "source": [
    "## 7.3.6 Using the pose graph\n",
    "\n",
    "Let's now use this whole structure we have built! We are going to use a `Robot` class to move around the environment. Every time we move, we will add an odometry node to the pose graph. Then, at arbitrary points of the exploration, we will simulate a loop closure (imagine we have run ICP and found a match between the point clouds of the sensor).\n",
    "\n",
    "Once we have a substantial graph, we'll run the optimization, and see if the loop closures are enough to correct the odometry errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd62b91",
   "metadata": {},
   "source": [
    "The robot is going to keep track of where it has been through a list of `KeyPose`. Each of these poses corresponds to one of the nodes in the graph, but also stores the readings of a laser scanner, which will be used to build the map once we have optimized the pose graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63270f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyPose:\n",
    "    def __init__(self, true_p, estimated_p, z):\n",
    "        self.true_pose = true_p\n",
    "        self.odom_pose = estimated_p\n",
    "        self.estimated_pose = estimated_p\n",
    "        self.z = z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5db56a",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 5: Exploring the environment</i></b></span>** \n",
    "\n",
    "Complete the `Robot` class below. Specifically, the methods `move_and_observe()` and `simulate_sensor_correspondence()`, which will add nodes and factors to the `PoseGraph` that we defined earlier.\n",
    "\n",
    "Note that, unlike in previous notebooks, the `move_and_observe()` method should apply the **correct** movement to the robot's true pose, and add a random error to the odometry. This simulates teleoperation: during a SLAM session, the user would be controllling the movement of the robot directly, and making sure it follows a specific path. However, the internal firmware of the robot, which handles the odometry, does not know what that path is, and can only reconstruct the movements from the imperfect odometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_map = LaserUtils.create_room_map()\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self, intial_pose):\n",
    "        self.true_pose : np.ndarray = intial_pose\n",
    "        self.estimated_pose : np.ndarray = intial_pose\n",
    "\n",
    "        self.factor_graph = PoseGraph()\n",
    "        self.factor_graph.nodes.append(Node(self.estimated_pose, fixed=True))\n",
    "\n",
    "        self.laser = Laser2D(np.deg2rad(240), 0.05, 20, np.diag([0.001,0.001]), np.vstack([0,0,0]))\n",
    "        self.history_poses : list[KeyPose] = []\n",
    "        self.take_observation()\n",
    "\n",
    "    def move_and_observe(self, movement, odom_cov):\n",
    "        odom = None\n",
    "        self.true_pose = None\n",
    "        self.estimated_pose = None\n",
    "\n",
    "        add_odom_factor(self.factor_graph, None, None, None)\n",
    "        self.take_observation()\n",
    "    \n",
    "    def take_observation(self):\n",
    "        self.laser.set_pose(self.true_pose)\n",
    "        z = self.laser.take_observation(sim_map)\n",
    "        self.history_poses.append(KeyPose(self.true_pose, self.estimated_pose, z))\n",
    "        \n",
    "    def simulate_sensor_correspondence(self, measurement_covariance, pose1_index=None, pose2_index=None):\n",
    "        # if optional indices are missing, just assume we are referring to the most recent poses\n",
    "        if pose1_index is None: # don't change this!\n",
    "            pose1_index = len(self.history_poses)-1\n",
    "        if pose2_index is None: # don't change this!\n",
    "            pose2_index = len(self.history_poses)-2\n",
    "\n",
    "        # find the relative transformation between the specified poses\n",
    "        ideal_transformation = inv_tcomp(self.history_poses[pose1_index].true_pose,\n",
    "                          self.history_poses[pose2_index].true_pose)\n",
    "        \n",
    "        noisy_transformation = None\n",
    "        \n",
    "        # create a new factor in the graph between the nodes\n",
    "        add_loop_closure(self.factor_graph, None, None, None, None)\n",
    "\n",
    "    def visualize(self, fig, ax):\n",
    "        DrawRobot(fig, ax, self.true_pose, color=\"red\")\n",
    "        DrawRobot(fig, ax, self.estimated_pose, color=\"blue\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba6afe",
   "metadata": {},
   "source": [
    "### Running it!\n",
    "\n",
    "Ok, time to see the pose graph optimization in action! Run the code in the cell below to see the path followed by the robot and the odometry estimation. Once the robot stops moving, the solver will try to improve on this odometry estimation by using the loop closures that we simulate along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_slam_optimization(robot: Robot):\n",
    "    # Solver methods: L-BFGS-B, Powell, SLSQP, COBYQA, trust-constr\n",
    "    run_solver(robot.factor_graph, method='Powell')\n",
    "    \n",
    "    # copy the results of the optimization to the robot's internal list of poses\n",
    "    for i in range(len(robot.factor_graph.nodes)):\n",
    "        robot.history_poses[i].estimated_pose = robot.factor_graph.nodes[i].pose\n",
    "\n",
    "\n",
    "# Dummy class used to indicate that we want a loop closure to happen \n",
    "class LoopClosure:\n",
    "    pass\n",
    "\n",
    "fig, ax = LaserUtils.plot_virtual_map(sim_map);\n",
    "clear_output(wait=False)\n",
    "\n",
    "robot = Robot(np.vstack([4, 4, 0]))\n",
    "robot.factor_graph.set_output(fig, ax)\n",
    "robot.visualize(fig,ax)\n",
    "movement_cov = np.diag([0.1, 0.02, 0.02])\n",
    "\n",
    "moves= [\n",
    "    np.vstack([1.0, 0., 0.]),\n",
    "    np.vstack([1.0, 0., 0.]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    LoopClosure(),\n",
    "\n",
    "    np.vstack([1.0, 0., 0.]),\n",
    "    np.vstack([1.0, 0., 0.]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    LoopClosure(),\n",
    "\n",
    "    np.vstack([1.2, 0., 0.]),\n",
    "    np.vstack([1.2, 0., 0.]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    LoopClosure(),\n",
    "\n",
    "    np.vstack([1.0, 0., 0.]),\n",
    "    np.vstack([1.0, 0., 0.]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    np.vstack([1.0, 0., np.pi / 4]),\n",
    "    LoopClosure()\n",
    "]\n",
    "\n",
    "np.random.seed(seed=233425)\n",
    "for i in range(len(moves)):\n",
    "    move = moves[i]\n",
    "    if isinstance(move, LoopClosure):\n",
    "        measurement_covariance = np.diag([0.005, 0.005, 0.002])\n",
    "        robot.simulate_sensor_correspondence(measurement_covariance, pose2_index=0)\n",
    "    else:\n",
    "        robot.move_and_observe(move, movement_cov)\n",
    "        robot.visualize(fig,ax)\n",
    "        robot.factor_graph.show()\n",
    "        time.sleep(0.2)\n",
    "\n",
    "run_slam_optimization(robot)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79442fec",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "<figure style=\"text-align:center; width:700px\">\n",
    "  <img src=\"images/posegraph/optimized_graph.png\" alt=\"\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db1455",
   "metadata": {},
   "source": [
    "Nice! With this pose graph optimization, we have managed to significantly improve the localization of the robot. Now that we know where the robot has been, we can use its sensory readings to build the map. Borrowing some code from the previous notebooks, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ef4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map_odometry():\n",
    "    map_width = np.nanmax(sim_map[0, :])\n",
    "    map_height = np.nanmax(sim_map[1, :])\n",
    "    occupancy_map = OccupancyGridMap(map_width, map_height, 0.1)\n",
    "\n",
    "    for pose in robot.history_poses:\n",
    "        process_new_observation(occupancy_map, pose.odom_pose, pose.z)\n",
    "    plot_grid_map(occupancy_map)\n",
    "\n",
    "def build_map_corrected():\n",
    "    map_width = np.nanmax(sim_map[0, :])\n",
    "    map_height = np.nanmax(sim_map[1, :])\n",
    "    occupancy_map = OccupancyGridMap(map_width, map_height, 0.1)\n",
    "\n",
    "    for pose in robot.history_poses:\n",
    "        process_new_observation(occupancy_map, pose.estimated_pose, pose.z)\n",
    "    plot_grid_map(occupancy_map)\n",
    "\n",
    "print(\"Using only odometry:\")\n",
    "build_map_odometry()\n",
    "print(\"With optimized pose graph:\")\n",
    "build_map_corrected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d1afb",
   "metadata": {},
   "source": [
    "As we can see, the pose graph allows us to get a quite good map, while blindly trusting odometry results on a very bad one. While the \"good\" map is still not perfect, we know that, in general, taking more observations would allow us to refine its imperfections by integrating their information in our probabilistic filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df162107",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (2)</i></b></font>\n",
    "\n",
    "Before we finish, let's **consider the following questions**:\n",
    "\n",
    "- Can you explain, in simple terms, how defining a pose graph helps us solve the SLAM problem?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>  \n",
    "\n",
    "- In this notebook, we only ran the mapping algorithm at the end of the process. That doesn't seem very *simultaneous*, does it. Can you think of how the algorithm might work if we needed to always have the most up-to-date map accessible to the robot?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>  \n",
    "\n",
    "- What happens if we change the number (or frequency) of loop closures? What if we change the covariance of the relative transformation that those loop closures find between the poses?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
