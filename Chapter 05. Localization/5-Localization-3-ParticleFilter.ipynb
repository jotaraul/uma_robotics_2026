{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07dfcdc",
   "metadata": {},
   "source": [
    "# 5.3 Particle Filter for Robot Localization\n",
    "\n",
    "When dealing with problems of a stochastic nature (meaning, the processes involved are random), there are two main approaches:\n",
    "\n",
    "- The first approach is to work **analytically**, with an explicitly defined probability distribution (.e.g., a Gaussian one), and estimate the parameters of that distribution. This is what we did with the Extended Kalman Filter.\n",
    "\n",
    "- The second one is to generate many **random samples**, and follow what happens to each of them as they are subjected to the randomness of the process. Crucially, each of these samples represents a single, determined value, rather than a probability distribution, and is thus easier to work with. Techniques using this second approach are often referred to as *Monte Carlo Methods*, and are very common in many disciplines. \n",
    "\n",
    "In this chapter, we will see a sample-based technique for probabilistic estimation, and apply it to the problem of robot localization. This technique goes by multiple names, such as *Monte Carlo Localization*, but we will refer to it with the generic term of **Particle Filter**. Specifically, we will:\n",
    "- Get an intuition about what a particle is (section 5.3.1),\n",
    "- Implement the building blocks of the filter (section 5.3.2), and\n",
    "- Put them all together to perform localization (section 5.3.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79ec7d",
   "metadata": {},
   "source": [
    "## 5.3.1 But, what is a particle?\n",
    "\n",
    "If we want to represent a probability distribution with a finite set of discrete values, we can do this by repeatedly sampling the distribution. In the context of Monte Carlo methods, that is precisely how we generate particles: by drawing samples from a distribution. \n",
    "\n",
    "What makes it a **particle**, rather than just a **sample**, is that a particle will later be transformed by *other* random samples. That is, a particle holds state, and that state can evolve over time. Let's see a toy example to solidify our understanding of this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379feca",
   "metadata": {},
   "source": [
    "### Basic example\n",
    "\n",
    "Let's say we have a random variable representing a position $[x,y]^T$, whose PDF is a 2D gaussian. We want to modify this position by moving a certain amount to the right, but the movement itself has some uncertainty, also described in this case by a gaussian. \n",
    "\n",
    "Now, imagine that you do not know how to propagate the uncertainty from these random variables to estimate the PDF of the result (even though, you of course do know!). How could we get a numerical approximation of this PDF, which we don't know how to calculate analytically? Well, with particles! Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import copy\n",
    "\n",
    "def basic_particles_example():\n",
    "    num_particles = 10000\n",
    "    start_mu = np.vstack([1, 1])\n",
    "    start_sigma = np.diag([1, 1])\n",
    "\n",
    "    # generate the particles by sampling the initial distribution\n",
    "    particles = stats.multivariate_normal(start_mu.flatten(), start_sigma).rvs(num_particles)\n",
    "    plt.scatter(particles[:,0], particles[:,1], s=0.1, color='blue')\n",
    "    print(\"initial distribution: mean=({:.2f}, {:.2f}), variances=({:.2f}, {:.2f})\".format(\n",
    "        np.mean(particles[:,0]), np.mean(particles[:,1]), \n",
    "        np.var(particles[:,0]), np.var(particles[:,1])))\n",
    "\n",
    "\n",
    "    colors= ['red', 'magenta', 'green']\n",
    "    for i in range(3):\n",
    "        # apply to each particle a single, random movement\n",
    "        movement_mu = np.vstack([10, 0])\n",
    "        movement_sigma = np.diag([2, 1])\n",
    "        movement = stats.multivariate_normal(movement_mu.flatten(), movement_sigma).rvs(num_particles)\n",
    "\n",
    "        particles += movement\n",
    "\n",
    "        # plot the result\n",
    "        plt.scatter(particles[:,0], particles[:,1], s=0.1, color=colors[i])\n",
    "        print(\"Distribution after movement {:d}: mean=({:.2f}, {:.2f}), variances=({:.2f}, {:.2f})\".format(\n",
    "            i+1,\n",
    "            np.mean(particles[:,0]), np.mean(particles[:,1]), \n",
    "            np.var(particles[:,0]), np.var(particles[:,1])))\n",
    "    \n",
    "\n",
    "basic_particles_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9517a2",
   "metadata": {},
   "source": [
    "Hey, that's pretty good! Turns out we can propagate uncertainty just by drawing many samples for each movement, and keeping track of where each of the particles ends up after moving.\n",
    "\n",
    "However, while this is an easy way of propagating uncertainty, we have the same problem we have had in the past: the uncertainty always gets larger, so we end up not knowing where we are at all. To correct this, and bring the uncertainty back down, we need to use sensory data (**observations**) to evaluate how good each particle is. \n",
    "\n",
    "Basically, the question we want ask is: *if the robot was at the position represented by this particle, how well does my existing observation match what I would expect to see?* Once we know this, we could remove very bad particles, and focus on the areas where good particles were found. That, in essence, is what the Particle Filter algorithm is.\n",
    "\n",
    "In the rest of this chapter, we will see an example of how to do this with a **laser scanner**, using the **likelihood field** model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a62b3c",
   "metadata": {},
   "source": [
    "### Setting things up: occupancy map and likelihood field\n",
    "\n",
    "Let's create a map, and use it to generate a **likelihood field**. Don't worry about the implementation details of this part, the important thing is what we get at the end: an occupancy map, and a likelihood field generated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e77032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.laser.laser2D import Laser2D\n",
    "from utils.DrawRobot import DrawRobot\n",
    "from utils.tcomp import tcomp\n",
    "from utils.RemapRange import RemapRange\n",
    "from utils.PlotEllipse import PlotEllipse\n",
    "from laserUtils import LaserUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad624af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    def __init__(self, contours_map : np.ndarray, resolution : float):\n",
    "        self.MIN_LIKELIHOOD = 0.1 # minimum value allowed in the likelihood field. Must be >0\n",
    "\n",
    "        self.min_x, self.min_y = (np.nanmin(contours_map[0]), np.nanmin(contours_map[1]))\n",
    "        self.max_x, self.max_y = (np.nanmax(contours_map[0]), np.nanmax(contours_map[1]))\n",
    "\n",
    "        self.__margin_size = 1\n",
    "        self.occupancy_img = np.full([int((self.max_y + self.__margin_size) / resolution), \n",
    "                       int((self.max_x + self.__margin_size) / resolution)],\n",
    "                      255, np.uint8)\n",
    "        \n",
    "        self.width, self.height = self.occupancy_img.shape\n",
    "        self.resolution = resolution\n",
    "\n",
    "        # create dense image from contours\n",
    "        #--------------------------------\n",
    "        # split into multiple contours and arrange into a list of 1x2 vectors\n",
    "        \n",
    "        # split into a different array every time there is a nan\n",
    "        indices = np.unique(np.where(np.isnan(contours_map))[1])\n",
    "        contours = np.hsplit(contours_map, indices) \n",
    "        contours = [c[~np.isnan(c)].reshape(2, -1) for c in contours] # get rid of the actual nans, keeping the 2D shape\n",
    "\n",
    "        # remove empty contours\n",
    "        contours = [c for c in contours if c.size > 0]\n",
    "        contours_formatted = [(c / resolution).astype('int32').T.reshape(-1, 1, 2) for c in contours]\n",
    "        \n",
    "        # draw the contours on the image\n",
    "        cv2.polylines(self.occupancy_img, contours_formatted, False, (0,0,0))\n",
    "\n",
    "        # calculate the likelihood field with a distance transform\n",
    "        #--------------------------------\n",
    "        self.likelihood_field = cv2.distanceTransform(self.occupancy_img, cv2.DIST_L2, 5)\n",
    "\n",
    "        # calculate likelihood from the pixel distances\n",
    "        SIGMA_HIT_M = 0.5 # set this to control how fast the likelihood decays as you get farther from the object\n",
    "        sigma_hit_px = SIGMA_HIT_M / resolution\n",
    "\n",
    "        self.likelihood_field = stats.norm(loc=0, scale=sigma_hit_px).pdf(self.likelihood_field)\n",
    "        \n",
    "        # remaping the values here to avoid having too-low values which will cause numerical issues upon repeated multiplication\n",
    "        # the actual value here is therefore *proportional* to the likelihood, not the likelihood itself\n",
    "        # but that's ok, because we will normalize the particle weights later :)\n",
    "        self.likelihood_field = RemapRange(self.likelihood_field, \n",
    "                                           0, np.max(self.likelihood_field),\n",
    "                                           self.MIN_LIKELIHOOD, 1)\n",
    "\n",
    "    def sample_likelihood(self, point : np.ndarray):\n",
    "        if point[0] < self.min_x or point[0] >= self.max_x or point[1] < self.min_y or point[1] >= self.max_y:\n",
    "            return self.MIN_LIKELIHOOD\n",
    "        indices = np.rint(np.flip(point / self.resolution, 0)).astype('int32')\n",
    "        value = self.likelihood_field[tuple(indices)]\n",
    "        return value\n",
    "    \n",
    "    def is_valid_position(self, point: np.ndarray):\n",
    "        if point[0] < self.min_x or point[0] >= self.max_x or point[1] < self.min_y or point[1] >= self.max_y:\n",
    "            return False\n",
    "        indices = np.rint(np.flip(point / self.resolution, 0)).astype('int32')\n",
    "        value = self.occupancy_img[tuple(indices)]\n",
    "        return value[0] > 0\n",
    "\n",
    "\n",
    "    # Visualization\n",
    "    #-------------------------------------\n",
    "    def show_occupancy(self):\n",
    "        return self.__print_img(self.occupancy_img)\n",
    "\n",
    "    def show_likelihood(self):\n",
    "        return self.__print_img(self.likelihood_field)\n",
    "    \n",
    "    def __print_img(self, img):\n",
    "        size_factor = 0.7\n",
    "        fig, ax = plt.subplots(figsize=(self.max_x * size_factor, self.max_y * size_factor))\n",
    "        img = cv2.flip(img, 0) # images are printed with the origin at the top, so flip vertically\n",
    "\n",
    "        ax.imshow(img, cmap='gray', extent=[0, self.max_x + self.__margin_size, 0, self.max_y + self.__margin_size])\n",
    "        plt.xlim([0, self.max_x+self.__margin_size])\n",
    "        plt.ylim([0, self.max_y+self.__margin_size])\n",
    "        \n",
    "        # Title and axis labels\n",
    "        plt.xlabel('X position (m)')\n",
    "        plt.ylabel('Y position (m)')\n",
    "        return fig, ax\n",
    "    \n",
    "\n",
    "# Create the map from the contours\n",
    "virtual_map = LaserUtils.create_room_map()\n",
    "image_resolution = 0.01\n",
    "occupancy_map = Map(virtual_map, image_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97369d74",
   "metadata": {},
   "source": [
    "That is a lot of code, but don't worry. The result of all that is just two maps:\n",
    "\n",
    "- The occupancy map (with object surfaces only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_map.show_occupancy();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d89633",
   "metadata": {},
   "source": [
    "- And the likelihood field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab842bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_map.show_likelihood();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b950d8",
   "metadata": {},
   "source": [
    "The likelihood field expresses a concept that can be a bit abstract for each point in the environment (a pixel in the image): when we take an observation with our laser scanner, what is the probability that one of the points in the point cloud will fall here? The intuition that this map codifies is that, while the sensor might be unreliable due to noise in the measurement, the measured points should still be *close* to the actual surfaces of the obstacles. The further away from the objects, the less likely we are to observe anything there.\n",
    "\n",
    "Formally, this can be expressed as $p(z^w_i | m)$, where $z^w_i$ is a point in the world-space point cloud $z^w$, and $m$ is the map. Crucially, this expression does not depend on the pose of the robot, $x$, so it is just a function of the geometry of the map and the reliability of the sensor. \n",
    "\n",
    "It should be pointed out that the likelihood field is a somewhat simplified model: it does not take into consideration the plausibility of each laser beam trajectory or from how many viewpoints each obstacle could be observed. Still, the image above is a reasonable enough approximation of $p(z^w | m)$, and gives good results in practice while being quite easy to generate.\n",
    "\n",
    "Using the likelihood field, we can now answer our previous question: if the robot was *here*, then how well does my sensor observation fit the map?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33109805-b23b-4f8a-9d78-bcff1cf0c6cc",
   "metadata": {},
   "source": [
    "## 5.3.2 Implementing the Particle Filter building blocks\n",
    "\n",
    "We will now look at each of the steps that go into creating the Particle Filter algorithm, individually:\n",
    "\n",
    "- Evaluating the particles.\n",
    "- Using the evaluations to estimate the real pose.\n",
    "- Propagating particles between iterations (sampling).\n",
    "- Replacing bad particles with better ones (resampling).\n",
    "\n",
    "Then, once we have collected all the pieces, we'll assemble the puzzle and get the full picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0041d27-20e2-49fe-9991-30d26b48930e",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Evaluating particles</i></b></span>**  \n",
    "\n",
    "Let's implement a function that takes in a map $m$ and a point cloud $z^w$, which represents the endpoints of a scan captured by a 2D laser scanner expressed in world coordinates, and returns the likelihood $p(z^w|m)$. The calculation we have to do is this:\n",
    "\n",
    "$$\n",
    "    p(z^w | m) \\propto \\prod_i p(z^w_i | m)\n",
    "$$\n",
    "\n",
    "Remember that $p(z^w_i | m)$ is precisely what the likelihood field represents. The expression above has a proportional-to sign rather than an equal sign,  but do not worry. That simply means that there are some normalization terms involved that we are not going to calculate directly. Instead, we will do a normalization step later on, to make sure the probabilities add up to 1. \n",
    "\n",
    "To make things easier, we provide you with the function `LaserUtils.express_as_global_point_cloud(z, pose)`, which does something we already saw in the sensing chapter: takes the sensor reading expressed in local space and polar coordinates, and turns it into a global-space, cartesian point cloud (which is what you need to sample the likelihood field).\n",
    "\n",
    "The map class also provides a useful function: `sample_likelihood()`, which returns the value of the likelihood field at a specific coordinate of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1987477-fa01-447a-8b5a-d4ed7a2c89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pointcloud(map : Map, pointcloud): \n",
    "    total_prob = 1\n",
    "    \n",
    "    for i in range(pointcloud.shape[1]):\n",
    "        None # calculate the likelihood for this point and accumulate using the formula in the text of the assignment\n",
    "\n",
    "    return total_prob\n",
    "\n",
    "def evaluate_pose(map: Map, z, pose):\n",
    "    world_pointcloud = None\n",
    "    return evaluate_pointcloud(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7d194-5458-415c-8ffa-d1a27d533fd3",
   "metadata": {},
   "source": [
    "Now check that your function is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9b9f2-df57-4241-83df-1000cd3ce926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_pointcloud():\n",
    "    pose =  np.vstack([4,4,0])\n",
    "    laser = Laser2D(np.pi, 0.1, 10, np.diag([0.01, 0.0]), pose)\n",
    "    z = laser.take_observation(virtual_map)\n",
    "\n",
    "    likelihood = evaluate_pose(occupancy_map, z, pose)\n",
    "    print(f\"Likelihood of point cloud: {likelihood}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = occupancy_map.show_likelihood()\n",
    "    DrawRobot(fig, ax, pose, linewidth=2)\n",
    "    LaserUtils.plot_observations(z, pose, ax, draw_lines=False)\n",
    "\n",
    "# Set a seed for random numbers\n",
    "np.random.seed(42)\n",
    "\n",
    "test_evaluate_pointcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc5353-bfa0-47e8-a021-f5bb2517fd80",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "Likelihood of point cloud: 0.7653936384712728\n",
    "```\n",
    "\n",
    "<img src=\"images/likelihood_field_with_scan.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c10d9f",
   "metadata": {},
   "source": [
    "Ok, that gives us a number for how believable this point cloud is, but that's not terribly useful on its own. The point of all this is that we can now **compare** point clouds to each other, and see how likely each of them are. \n",
    "\n",
    "Let's remind ourselves why we want to do this. The idea is that, given a sensor reading $z_c$ (in cartesian coordinates), in order to get a global-space point cloud $z^w$ we need to know the pose of the robot $x$ so:\n",
    "\n",
    "$$ z^w = x \\oplus z_c $$\n",
    "\n",
    "If we have multiple candidate poses (particles), we can use each of them to reproject the readings and get a point cloud. If we now evaluate those point clouds, that is equivalent to evaluating the poses themselves.\n",
    "\n",
    "And that is what we want! We want to test many possible poses, and use that to estimate what the real pose is.\n",
    "\n",
    "Let's take that next step, then. Given a sensor reading and few candidate poses (particles), evaluate each pose with your brand new ``evaluate_pointcloud()`` function. Think about why each pose receives the evaluation that it gets, given that you can also see the real pose that the measurement was taken from (as usual, in red). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7881f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotParticle(fig, ax, pose, z, color):\n",
    "    DrawRobot(fig, ax, pose, 0.01, linewidth=2, color=color)\n",
    "    LaserUtils.plot_observations(z, pose, ax, draw_lines=False, point_color=color)\n",
    "\n",
    "def test_particle_evaluation():\n",
    "    true_pose =  np.vstack([4,4,0])\n",
    "    laser = Laser2D(np.pi, 0.1, 10, np.diag([0.01, 0.0]), true_pose)\n",
    "    z = laser.take_observation(virtual_map)\n",
    "\n",
    "    pose_particle1 = np.vstack([4.2, 4.4, 0.1])\n",
    "    pose_particle2 = np.vstack([4.01, 4.1, -0.1])\n",
    "    pose_particle3 = np.vstack([4.4, 3.8, -0.2])\n",
    "\n",
    "    # plot\n",
    "    fig, ax = occupancy_map.show_likelihood()\n",
    "    DrawRobot(fig,ax, true_pose, 0.01, linewidth=2, color=\"red\")\n",
    "    PlotParticle(fig, ax, pose_particle1, z, \"orangered\")\n",
    "    PlotParticle(fig, ax, pose_particle2, z, \"olive\")\n",
    "    PlotParticle(fig, ax, pose_particle3, z, \"magenta\")\n",
    "\n",
    "    # evaluate the particles\n",
    "    eval_1 = None\n",
    "    eval_2 = None\n",
    "    eval_3 = None\n",
    "\n",
    "    #print evaluations\n",
    "    print(f\"Particle 1 (orange): {eval_1}\")\n",
    "    print(f\"Particle 2 (green): {eval_2}\")\n",
    "    print(f\"Particle 3 (magenta): {eval_3}\")\n",
    "\n",
    "test_particle_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e5611-8d3f-4102-88be-950454a82100",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "Particle 1 (orange): 9.697793288623095e-06\n",
    "Particle 2 (green): 0.2699737678772269\n",
    "Particle 3 (magenta): 1.2663658417295267e-06\n",
    "```\n",
    "\n",
    "<img src=\"images/likelihood_field_with_particle_evaluations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48faeea5-fc89-46ac-a94d-9405169e175d",
   "metadata": {},
   "source": [
    "### Pose estimation from particles\n",
    "\n",
    "So, if we have a list of particles representing possible values for the pose of the robot, and we can evaluate how good each of them are, how do we turn that into a single estimation of the *real* pose?\n",
    "\n",
    "Probability theory gives us the perfect tool for this: the **expected value**. This is the same as the mean of a probability distribution, and can be very easily computed as a weighted average, where the weights are the probabilities of each value:\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_x p(x) \\cdot x\n",
    "$$\n",
    "\n",
    "Since we don't quite have a probability distribution, but instead a set of particles, our computation is instead:\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_i w_i \\cdot x_i\n",
    "$$\n",
    "\n",
    ", where $w_i$ is the weight of particle $i$, and $x_i$ is the pose it represents. \n",
    "\n",
    "We can also calculate the uncertainty of our current particle distribution by using this expected value to get the covariance matrix associated with our estimated pose. The definition of the covariance between two variables $X$ and $Y$ is as follows:\n",
    "\n",
    "$$\n",
    "cov_{XY} = E[(X-E[X]) \\cdot (Y-E[Y])]\n",
    "$$\n",
    "\n",
    "So, in our case:\n",
    "\n",
    "$$\n",
    "cov_{XY} = \\sum_i w_i \\cdot [(x_i-E[X]) \\cdot (y_i-E[Y])]\n",
    "$$\n",
    "\n",
    "For example, this means that: \n",
    "\n",
    "$$\n",
    "\\sigma^2_x = \\sum_i w_i \\cdot (x_i-E[X])^2 \n",
    "$$\n",
    "$$\n",
    "\\sigma^2_y = \\sum_i w_i \\cdot (y_i-E[X])^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659014f2",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Estimating the real pose from the particle evaluations</i></b></span>**  \n",
    "\n",
    "Let's implement a function, `current_estimation()`, which receives a list of pre-evaluated particles and returns the expected value of the pose, along with the covariance of that estimation. Since averaging angles is a little tricky, and we don't expect you to be an expert on circular statistics, you just need to handle the *position* part of the pose.\n",
    "\n",
    "You are provided with a `Particle` class, which holds the pose which a particle represents and the weight it was assigned by the likelihood evaluation in the previous step. There is also an already implemented `normalize_weights()` function, which makes sure all the weights add up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe06f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, pose, weight):\n",
    "        self.pose = pose\n",
    "        self.weight = weight\n",
    "\n",
    "# This just makes it so the sum of all the weights is equal to 1\n",
    "def normalize_weigths(particles):\n",
    "    total_weight = 0.\n",
    "    for particle in particles:\n",
    "        total_weight += particle.weight\n",
    "\n",
    "    for particle in particles:\n",
    "        particle.weight /= total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_estimation(particles):\n",
    "    normalize_weigths(particles)\n",
    "    \n",
    "    # we cannot just average angles, so instead we separate position and heading direction\n",
    "    # averaging the (vector) heading directions is the most common definition of \"circular mean\" (https://en.wikipedia.org/wiki/Circular_mean)\n",
    "    expected_value_position = np.vstack([0., 0.])\n",
    "    expected_value_direction = np.vstack([0., 0.])\n",
    "\n",
    "    # calculate the expected values (weighted average)\n",
    "    for particle in particles:\n",
    "        position = None\n",
    "        angle = None\n",
    "\n",
    "        expected_value_position += None\n",
    "        expected_value_direction += np.vstack([np.cos(angle), np.sin(angle)]) * particle.weight\n",
    "\n",
    "    # get the angle value from the averaged directions\n",
    "    expected_angle = np.atan2(expected_value_direction[1], expected_value_direction[0]) \n",
    "    expected_angle = np.vstack([expected_angle])\n",
    "\n",
    "    # Calculate the expected value of the robot pose\n",
    "    expected_pose = np.concatenate([expected_value_position, expected_angle], 0)\n",
    "\n",
    "    # Calculate the diagonal elements of the covariance matrix (we will ignore the rest to avoid dealing with angular differences)\n",
    "    var_x = 0.\n",
    "    var_y = 0.\n",
    "    for particle in particles:\n",
    "        var_x += None\n",
    "        var_y += None\n",
    "\n",
    "    # circular variance is inversely proportional to the length of the circular mean vector\n",
    "    angular_variance = 1 - np.linalg.norm(expected_value_direction)\n",
    "    covariance = np.diag([var_x, var_y, angular_variance])\n",
    "    return expected_pose, covariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe97a6f",
   "metadata": {},
   "source": [
    "Let's use this, then. We'll create a few random particles and evaluate each one. Then, we will look at those evaluations to extract the expected value and covariance of the estimated pose of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_estimate_pose():\n",
    "    true_pose =  np.vstack([4,4,0])\n",
    "    laser = Laser2D(np.pi, 0.1, 10, np.diag([0.01, 0.0]), true_pose)\n",
    "    z = laser.take_observation(virtual_map)\n",
    "    \n",
    "    num_particles = 300\n",
    "    particles = []\n",
    "    for i in range(num_particles):\n",
    "        # uniformly distributed random particles in the general vecinity of the real pose\n",
    "        pose_particle = np.vstack([\n",
    "            (np.random.rand()-0.5) * 2 + true_pose[0],\n",
    "            (np.random.rand()-0.5) * 2 + true_pose[1],\n",
    "            (np.random.rand()-0.5) * 1 + true_pose[2],\n",
    "        ])\n",
    "        # get the evaluation for this random pose and add it to the list\n",
    "        weight = evaluate_pointcloud(occupancy_map, LaserUtils.express_as_global_point_cloud(z, pose_particle))\n",
    "        particles.append(Particle(pose_particle, weight))\n",
    "\n",
    "    # use the particles and their evaluations to estimate the real pose\n",
    "    estimated_pose, estimated_cov = current_estimation(particles)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = occupancy_map.show_likelihood()\n",
    "    DrawRobot(fig, ax, true_pose, 0.01, linewidth=2, color=\"red\")\n",
    "    DrawRobot(fig, ax, estimated_pose, 0.01, linewidth=2, color=\"green\")\n",
    "    PlotEllipse(fig, ax, estimated_pose[0:2], estimated_cov, color=\"limegreen\")\n",
    "\n",
    "test_estimate_pose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553dd07-7e1e-49b7-90ab-502405c0d8df",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "<img src=\"images/likelihood_field_with_estimated_pose.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5d36a",
   "metadata": {},
   "source": [
    "We can see the expected value does not always give us a perfect estimation, but it tends to be quite close. If we increase the number of particles, the estimations tend to be more reliable and the uncertainty is lower. Try different numbers of particles and check this for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fbf98",
   "metadata": {},
   "source": [
    "### Sampling and Resampling\n",
    "\n",
    "We know how to evaluate particles, and how to generate a final estimation from those particles. However, where do those particles come from? Well, initially, they are just generated around the starting pose of the robot. We can do this by just taking samples of the pose's probability distribution, like we did all the way at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620329fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_particles(num_particles, pose, covariance):\n",
    "    particles : list[Particle] = []\n",
    "    for i in range(num_particles):\n",
    "        particle_pose = np.vstack(stats.multivariate_normal.rvs(pose.flatten(), covariance))\n",
    "        particles.append(Particle(particle_pose, 1. / num_particles)) # equal weight to start with, we don't yet know how good these particles are\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da78fab-e05b-44d0-be3c-841e09d750f5",
   "metadata": {},
   "source": [
    "After that, there are two basic operations to generate particles: **sampling** (using the previous generation of particles and the control action), and **resampling** (just trying new random values). Let's see them.\n",
    "\n",
    "**Sampling** is easy enough: we take each existing particle from the previous iteration $x_{t-1}$, and we move it according to the movement $\\hat{u_t}$ that the robot tried to do. The key is that, since the movement has uncertainty, the particle actually receives a noisy movement, which doesn't perfectly correspond to the control action. This noisy movement is generated by using the covariance of the movement, $\\Sigma_u$, to sample its probability distribution. The final pose of the particle is then $$ x_t = x_{t-1} \\oplus u_t$$ , with $$u_t \\sim N(\\hat{u_t}, \\Sigma_u)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058b4d9",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Sampling particles</i></b></span>**  \n",
    "\n",
    "Complete the `sample()` function below, which takes a list of particles (representing hypotheses for the pose of the robot) and a movement command. The function then modifies each the particles to estimate the pose of the robot after making the movement.\n",
    "\n",
    "You need to also complete the `generate_random_movement()` function, which takes a movement command, expressed as a pose increment $u = [\\Delta x, \\Delta y, \\Delta \\theta]^T$ and its covariance, and generates a random movement from it. The random movement is generated using the sampling model discussed in notebook 3.3, where the movement is expressed as $u = [\\theta_1, d, \\theta_2]^T$:\n",
    "\n",
    "1. **Turn** ($\\theta_1$): to face the destination point.\n",
    "2. **Advance** ($d$): to arrive at the destination.\n",
    "3. **Turn** ($\\theta_2$): to get to the desired angle.\n",
    "\n",
    "Remember that you can convert between the two representations with the following formulas:\n",
    "\n",
    "\\begin{split}\n",
    "    & \\theta_1 =atan2(\\Delta y, \\Delta x) \\\\\n",
    "    & d = \\sqrt{(\\Delta x)^2 + (\\Delta y)^2} \\\\\n",
    "    & \\theta_2  = \\theta - \\theta_1 \n",
    "\\end{split}\n",
    "\n",
    "The jacobian for this conversion, which will be necessary to calculate this covariance, is:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    \\frac{- \\Delta y}{d^2} &  \\frac{\\Delta x}{d^2} & 0 \\\\\n",
    "    \\frac{\\Delta x}{d} &  \\frac{\\Delta y}{d} & 0 \\\\\n",
    "    \\frac{\\Delta y}{d^2} &   \\frac{- \\Delta x}{d^2} & 1\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_movement(ideal_movement, movement_covariance):\n",
    "    x = ideal_movement[0]\n",
    "    y = ideal_movement[1]\n",
    "    theta = ideal_movement[2]\n",
    "\n",
    "    # change the movement command to the angle-movement-angle mode to generate the random sample\n",
    "    a1 = np.arctan2(y, x)\n",
    "    d = np.sqrt(x*x + y*y)\n",
    "    a2 = theta - a1\n",
    "    mean_sampling = np.vstack([None, None, None])\n",
    "    \n",
    "    jacobian = np.array([\n",
    "        [-y/(d**2), x/(d**2), 0],\n",
    "        [x/d, y/d, 0],\n",
    "        [y/(d**2), x/(d**2), 1]\n",
    "    ])\n",
    "\n",
    "    cov_sampling = None @ None @ None\n",
    "    \n",
    "    movement_sample = np.vstack(stats.multivariate_normal.rvs(None, None))\n",
    "    \n",
    "    # revert to pose increment so we can do the composition\n",
    "    pose_increment_sample = np.vstack([\n",
    "        movement_sample[1] * np.cos(movement_sample[0]),\n",
    "        movement_sample[1] * np.sin(movement_sample[0]),\n",
    "        movement_sample[0] + movement_sample[2]\n",
    "    ])\n",
    "\n",
    "    return pose_increment_sample\n",
    "\n",
    "\n",
    "def sample(particles, ideal_movement, movement_covariance):\n",
    "    for particle in particles:\n",
    "        noisy_movement = generate_random_movement(ideal_movement, movement_covariance)\n",
    "        particle.pose = tcomp(particle.pose, noisy_movement)\n",
    "\n",
    "# function to plot the particles\n",
    "def show_particles(particles, fig, ax, size = 0.002, color=\"cyan\"):\n",
    "    for particle in particles:\n",
    "        DrawRobot(fig, ax, particle.pose, size, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332a863",
   "metadata": {},
   "source": [
    "Now test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68787379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample_function():\n",
    "    fig, ax = occupancy_map.show_occupancy()\n",
    "\n",
    "    ideal_movement = np.vstack([2, 0, 0])\n",
    "    movement_covariance = np.diag([0.02, 0.04, 0.02])\n",
    "    \n",
    "    # initial state\n",
    "    initial_pose = np.vstack([4,4,0])\n",
    "    initial_pose_cov = np.diag([0.05, 0.05, 0.04])\n",
    "    DrawRobot(fig, ax, initial_pose, linewidth=1.)\n",
    "    particles = generate_particles(20, initial_pose, initial_pose_cov)\n",
    "    show_particles(particles, fig, ax, size=0.007, color=\"orange\")\n",
    "    \n",
    "    # simulate one movement\n",
    "    sample(particles, ideal_movement, movement_covariance)\n",
    "    show_particles(particles, fig, ax, size=0.007, color=\"cyan\")\n",
    "\n",
    "    # simulate another movement\n",
    "    sample(particles, ideal_movement, movement_covariance)\n",
    "    show_particles(particles, fig, ax, size=0.007, color=\"limegreen\")\n",
    "\n",
    "test_sample_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f477fcc-3e5f-4390-adc2-bb9ca8809278",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "<img src=\"images/particle_filter_sampling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db523a1d",
   "metadata": {},
   "source": [
    "After doing the sampling step, we might decide that some of these particles are very bad, and thus not interesting. Rather than keep propagating these bad estimations, we would prefer to allocate our computational resources to evaluating candidate poses that are more likely to be close to the correct value. *That* is the idea behind **resampling**. We eliminate particles that got a low evaluation, and generate new particles to replace them. \n",
    "\n",
    "These new particles can be generated many different ways, but it is generally a good idea to use the current estimation of the pose (or the particles which got a good evaluation) as a reference value around which to spawn the new particles. This is a technique known as [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling), and it allows us to get a good estimation even with a relatively small number of particles, since we avoid wasting resources by creating particles that are very far away from the pose we already know is likely to be correct.\n",
    "\n",
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Resampling particles</i></b></span>**  \n",
    "\n",
    "Let's do just that, then. We will use the provided function `eliminate_bad_particles()` to get rid of the particles whose evaluation is below a certain threshold. Then, we'll use the `resample()` function, which you must complete, to generate new particles. \n",
    "\n",
    "The new particles must spawn around the current estimated pose, using the uncertainty of the estimation to draw the new samples. A new particle $i$ would then be generated by drawing a sample pose $x_i$:\n",
    "\n",
    "$$\n",
    "x_i \\sim N(E[X], \\Sigma)\n",
    "$$\n",
    "\n",
    "Optionally, one could \"scale up\" the uncertainty by an arbitrary factor $\\lambda$, to make sure the new particles generated cover a wider spectrum of hypothetical poses. This way, we can avoid having all the new particles be very similar to the old ones:\n",
    "\n",
    "$$\n",
    "x_i \\sim N(E[X], \\lambda \\cdot \\Sigma)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_bad_particles(particles, kill_weight_threshold):\n",
    "    # the simplest possible approach to eliminating particles: check their likelihood against a fixed threshold\n",
    "    is_good = lambda p : p.weight > kill_weight_threshold \n",
    "    return [p for p in particles if is_good(p)]\n",
    "\n",
    "# generate new particles around the current estimation for the robot pose\n",
    "# the more uncertainty we have, the more area we want the new particles to cover\n",
    "def resample(particles, desired_particle_count, map):  \n",
    "    SPAWN_RADIUS_MULTIPLIER = 2 # arbitrarily make the sampling area bigger to avoid getting stuck on a bad estimation \n",
    "    estimated_pose, estimated_covariance = current_estimation(particles)\n",
    "    sigma_x = np.sqrt(estimated_covariance[0,0]) * SPAWN_RADIUS_MULTIPLIER\n",
    "    sigma_y = np.sqrt(estimated_covariance[1,1]) * SPAWN_RADIUS_MULTIPLIER\n",
    "    sigma_angle = np.sqrt(estimated_covariance[2,2]) * SPAWN_RADIUS_MULTIPLIER\n",
    "\n",
    "    while len(particles) < desired_particle_count:\n",
    "        pose = np.vstack([\n",
    "            None\n",
    "        ])\n",
    "\n",
    "        if map.is_valid_position(pose[0:2]):\n",
    "            particles.append(None)\n",
    "    \n",
    "    # Technically, since we are doing a form of importance sampling (https://en.wikipedia.org/wiki/Importance_sampling),\n",
    "    # the resulting weights should not be uniform...\n",
    "    # but then, this is a simple example implementation :)\n",
    "    for particle in particles:\n",
    "        particle.weight = 1. / len(particles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc3fa5",
   "metadata": {},
   "source": [
    "Test the resampling with the following code. The particles that are eliminated will be shown in grey, the particles that survive will be shown in orange, and the newly sampled particles will be shown in cyan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751616ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_resampling():\n",
    "    fig, ax = occupancy_map.show_occupancy()\n",
    "\n",
    "    movement = np.vstack([2, 0, 0])\n",
    "    movement_covariance = np.diag([0.02, 0.04, 0.02])\n",
    "    \n",
    "    # initial state\n",
    "    initial_pose = np.vstack([4,4,0])\n",
    "    initial_pose_cov = np.diag([0.05, 0.05, 0.04])\n",
    "    particles = generate_particles(20, initial_pose, initial_pose_cov)\n",
    "    DrawRobot(fig, ax, initial_pose, linewidth=1.)\n",
    "\n",
    "    # move and sample\n",
    "    real_pose = tcomp(initial_pose, movement)\n",
    "    sample(particles, movement, movement_covariance)\n",
    "    DrawRobot(fig, ax, real_pose, linewidth=1., color=\"orange\")\n",
    "    show_particles(particles, fig, ax, size=0.007, color=\"grey\")\n",
    "    \n",
    "    # evaluate\n",
    "    laser = Laser2D(np.pi, 0.1, 10, np.diag([0.01, 0.0]), real_pose)\n",
    "    z = laser.take_observation(virtual_map)\n",
    "    for particle in particles:\n",
    "        particle.weight = evaluate_pointcloud(occupancy_map, LaserUtils.express_as_global_point_cloud(z, particle.pose))\n",
    "    normalize_weigths(particles)\n",
    "\n",
    "    # resample\n",
    "    particles = eliminate_bad_particles(particles, 1e-5)\n",
    "    show_particles(particles, fig, ax, size=0.007, color=\"orange\")\n",
    "\n",
    "    display(fig)        \n",
    "    clear_output(wait=True)\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    old_particles = copy.deepcopy(particles)\n",
    "    resample(particles, 20, occupancy_map)\n",
    "    show_particles(particles, fig, ax, size=0.007, color=\"cyan\")\n",
    "    show_particles(old_particles, fig, ax, size=0.007, color=\"orange\")\n",
    "    display(fig)        \n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "test_resampling()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383be7a4-50ef-4aa5-bc0f-5e8caf9a82c0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "<img src=\"images/particle_filter_resampling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585d7d7-1d5f-424b-b52d-e336fc57996c",
   "metadata": {},
   "source": [
    "## 5.3.3 Assembling the Particle Filter\n",
    "\n",
    "We now have all the pieces of the particle filter, so let's put them in order and see the algorithm come to life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c2743",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 5: Putting all together</i></b></span>**  \n",
    "\n",
    "First, complete the body of the function`update_weights()` in the `ParticleFilter` class below. Remember that the weight of a particle at time instant $t$ is updated recursively based on the likelihood estimated for the most recent measurement (which we calculated in the evaluation section):\n",
    "\n",
    "$$\n",
    "w^i_t \\propto w^i_{t-1} \\cdot p(z^w_t | m)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleFilter:\n",
    "    def __init__(self, initial_pose):\n",
    "        # You can play with these parameters\n",
    "        self.min_particles = 30\n",
    "        self.max_particles = 70\n",
    "        self.kill_weight_threshold = 1e-3\n",
    "\n",
    "        num_particles_init = 50\n",
    "        self.particles = generate_particles(num_particles_init, initial_pose, np.diag([0,0,0]))\n",
    "\n",
    "    def update_weights(self, map, z):\n",
    "        for particle in self.particles:\n",
    "            None # update the particle weight using the observation\n",
    "\n",
    "        normalize_weigths(self.particles)\n",
    "\n",
    "\n",
    "    def refresh_particles(self, map):\n",
    "        self.particles = eliminate_bad_particles(self.particles, self.kill_weight_threshold)\n",
    "\n",
    "        # In a real implementation we would check a degeneracy metric rather than look at the particle count\n",
    "        if len(self.particles) < self.min_particles:\n",
    "            resample(self.particles, self.max_particles, map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2d26f",
   "metadata": {},
   "source": [
    "Now, let's run the particle filter algorithm! Complete the main loop below to estimate the pose of the robot as it moves around the environment. You will need to use the functions that we have defined previously in the notebook for each of the steps.\n",
    "\n",
    "The core process is as follows:\n",
    "\n",
    "- Apply the movement command to all particles (sampling).\n",
    "- Evaluate each candidate pose given the most recent measurement.\n",
    "- Calculate the estimated pose and its associated uncertainty.\n",
    "- Eliminate bad particles and (maybe) generate new ones (resampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea46854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_particle_filter():\n",
    "    true_pose = np.vstack([4,4,0])\n",
    "    ideal_pose = true_pose \n",
    "\n",
    "    movements = [\n",
    "        np.vstack([2, 0, 0]),\n",
    "        np.vstack([2, 0, 0]),\n",
    "        np.vstack([0, 2, np.pi/2]),\n",
    "        np.vstack([2, 0, 0]),\n",
    "        np.vstack([0, 2, np.pi/2]),\n",
    "        np.vstack([2, 0, 0]),\n",
    "    ]\n",
    "    cov = np.diag([0.02, 0.02, 0.01])\n",
    "\n",
    "    laser = Laser2D(np.pi, 0.1, 10, np.diag([0.01, 0.0]), true_pose)\n",
    "    pf = ParticleFilter(true_pose)\n",
    "\n",
    "    fig, ax = occupancy_map.show_occupancy()\n",
    "    DrawRobot(fig, ax, ideal_pose, 0.015, linewidth=1.5, color=\"red\")\n",
    "    plt.ion()\n",
    "\n",
    "    old_particles=[]\n",
    "\n",
    "    for move in movements:\n",
    "        # move and take a reading\n",
    "        ideal_pose = tcomp(ideal_pose, move)\n",
    "        DrawRobot(fig, ax, ideal_pose, 0.015, linewidth=1.5, color=\"red\")\n",
    "\n",
    "        true_pose = tcomp(true_pose, generate_random_movement(move, cov))\n",
    "        DrawRobot(fig, ax, true_pose, 0.015, linewidth=1.5, color=\"blue\")\n",
    "        \n",
    "        # take measurement\n",
    "        laser.set_pose(true_pose)\n",
    "        z = laser.take_observation(virtual_map)\n",
    "        \n",
    "        # MAIN EVENT: run the particle filter!\n",
    "        #-------------------------\n",
    "        sample(pf.particles, move, cov)\n",
    "        pf.update_weights(occupancy_map, z)\n",
    "        estimated_pose, estimated_covariance = current_estimation(pf.particles)\n",
    "        pf.refresh_particles(occupancy_map)\n",
    "        #-------------------------\n",
    "\n",
    "        # plot the estimation\n",
    "        DrawRobot(fig, ax, estimated_pose, 0.015, linewidth=1.5, color=\"green\")\n",
    "        # PlotEllipse(fig, ax, estimated_pose[0:2], estimated_covariance, color=\"green\")\n",
    "\n",
    "        # plot the active particles, and make the old ones a different color\n",
    "        show_particles(pf.particles, fig, ax, color = \"cyan\")\n",
    "        show_particles(old_particles, fig, ax, color = \"lightgrey\")\n",
    "        old_particles = copy.deepcopy(pf.particles)\n",
    "\n",
    "        # keep updating the plot on the same figure\n",
    "        display(fig)        \n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "test_particle_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0574e71-3cba-4752-b42c-ec8c6e220e61",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "<img src=\"images/particle_filter_localization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1d779",
   "metadata": {},
   "source": [
    "So that's the algorithm! As you can see, the process is conceptually much simpler than something like EKF, given we are always operating on specific poses rather than probability distributions, but there is a price to pay in terms of computational complexity and accuracy.\n",
    "\n",
    "Still, Monte Carlo methods are very powerful tools, and very commonly used. Their ability to represent any type of distribution (rather than being constrained to gaussian approximations) means they are very flexible. The implementation we have explored here is a relatively simplistic one, but making only a few changes you could represent multi-modal distributions, consider whether specific trajectories would collide with obstacles, *etc.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d7d66-6fe7-4f96-947b-73ce8f0bc61e",
   "metadata": {},
   "source": [
    "## <font color=\"orange\"><b><i>AI Appendix</i></b></font>\n",
    "\n",
    "### A.1 How I used AI\n",
    "- **Model/tool:**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **What I asked AI to help with (bullets):**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **Best prompt I used (paste):**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **What I kept vs. changed from the AI output:**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "### A.2 AI review \n",
    "Ask one generative AI to review your memo using this prompt:\n",
    "\n",
    "```\n",
    "Act as a technical reviewer of this computer vision lab notebook. Read my memo and return exactly 5 bullet points:\n",
    "\n",
    "- two on theory (key ideas),\n",
    "- two on practice (relevant programming concepts of python used, not particular functions or variables),\n",
    "- one on how to improve the code (most critical aspect: limitations, parameter choices, runtime/robustness, possible bugs).\n",
    "\n",
    "Keep each bullet 15 words and make them specific to my work.\n",
    "```\n",
    "\n",
    "Paste its **5** items here:\n",
    "\n",
    "- **(Theory 1)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Theory 2)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Practice 1)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Practice 2)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Improvement)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "Finally, **do you agree with them?**, **would you have selected others?** \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
