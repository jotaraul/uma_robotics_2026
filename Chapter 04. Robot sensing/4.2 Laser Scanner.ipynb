{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2892af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 4.2 Laser Scanners\n",
    "\n",
    "One of the most widely used types of range/bearing sensors is the 2D LiDAR (*Light-based Detection And Ranging*) or laser scanner. Their principle of operation is simple: the sensor emits a pulse of light of a particular frequency, and waits for it to be reflected back by the environment. Once the reflected pulse is received, the distance it has traveled can be calculated from the time-of-flight (TOF).\n",
    "\n",
    "\n",
    "A 2D scanner repeats this operation many times while rotating around its own center, thus obtaining information about the distance to the environment at different angles on its plane of operation. The main parameters that describe a laser scanner are:\n",
    "\n",
    "- The angle increment between one measurement and the next is the **angular resolution** of the scanner, and determines how detailed the complete measurement is.\n",
    "- The **field of view (FOV)** determines the angle limits of where the scan starts and stops. While some commercial LiDARs are capable of scanning all around themselves, it is often necessary to limit the FOV due to where the sensor is mounted on the robot (as we want to prevent the sensor from observing the body of the robot itself).\n",
    "- The **max distance**, past which the sensor is not able to produce a measurement at all.\n",
    "- The **noise**, of course. You know how this goes by now: sensor measurements are never fully reliable. Depending on the specifics of the sensor, the uncertainty of the measurements might only be reported for the *distance* reading (that is, a scalar **variance**), or for both the distance and the angle (a **covariance matrix**).  \n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"images/sick_and_hokuyo_laser_scanners.png\" width=\"600px\" alt=\"\">\n",
    "    <figcaption>Top row, 2D laser scanners from <a href=\"https://www.sick.com/es/es/catalog/productos/safety/escaner-laser-de-seguridad/c/g569359\" href=\"_blank\">SICK</a>. Bottom row, left, 2D laser scanners from <a href=\"https://www.hokuyo-aut.jp/search/?cate01=1&cate02=1&cate03=\" target=\"_blank\">Hokuyo</a>, right, illustration of the field of view of some models of this brand.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Let's create a simulated Laser Scanner with these parameters and test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0607f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.laser.laser2D import Laser2D\n",
    "from utils.DrawRobot import DrawRobot\n",
    "from utils.tcomp import tcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585af4e",
   "metadata": {},
   "source": [
    "### Creating the scanner and the map\n",
    "\n",
    "We will use the `Laser2D` class imported above. The constructor of this class expects the parameters we just discussed:\n",
    "- `FOV` -- Sensor field of view (radians)\n",
    "- `resolution` -- Sensor resolution (radians)\n",
    "- `max_distance` -- Max operating distance of the sensor (meters)\n",
    "- `noise_cov` -- covariance matrix characterizing sensor noise\n",
    "- `pose` -- sensor pose (column vector with x and y positions, and orientation theta)\n",
    "\n",
    "The following function shows you how to create an instance of the `Laser2D` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_laser_at_origin(FOV, resolution, max_distance, noise_cov):\n",
    "    origin_pose = np.vstack([0, 0, 0])\n",
    "    laser = Laser2D(FOV, resolution, max_distance, noise_cov, origin_pose)\n",
    "    return laser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7c5d3",
   "metadata": {},
   "source": [
    "The `Laser2D` class has two important methods: `set_pose()`, to tell it where to take an observation from; and `take_observation()`, which does exactly what you think. However, you might be asking \"an observation of *what*, exactly? We haven't told the laser what the environment is like.\" Well, you're right, so let's do just that!\n",
    "\n",
    "We will define the map as a list of polygonal **contours**. In the following cell, you will see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0730d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_room_map():\n",
    "    # Define the environment\n",
    "    # A polygon contour is defined as two arrays, specifying the x and y coordinates of each point\n",
    "    # The polygon is generated by connecting (x_t, y_t) to (x_t+1, y_t+1) with a straight line segment\n",
    "\n",
    "    # NaN denotes the end of each polygon\n",
    "    walls = np.array([[2, 12, 12, 10, 10, 9.5, 9.5, 2, 2, np.nan],\n",
    "                        [2, 2, 10, 10, 8, 8, 10, 10, 2, np.nan]])\n",
    "\n",
    "    kitchen = np.array([[2, 8.5, 8.5, 9.5, 9.5, 2, 2, np.nan],\n",
    "                        [9, 9, 8, 8, 10, 10, 9, np.nan]])\n",
    "\n",
    "    sofa = np.array([[3.5, 7, 7, 3.5, 3.5, np.nan],\n",
    "                        [6, 6, 7, 7, 6, np.nan]])\n",
    "\n",
    "    tv_cabinet = np.array([[2.5, 7.5, 7.5, 2.5, 2.5, np.nan],\n",
    "                            [2, 2, 3, 3, 2, np.nan]])\n",
    "\n",
    "    table = np.array([[9.5, 11, 11, 9.5, 9.5, np.nan],\n",
    "                        [3.5, 3.5, 6, 6, 3.5, np.nan]])\n",
    "\n",
    "\n",
    "    virtual_map = np.concatenate([walls, kitchen, sofa, tv_cabinet, table], axis=1)\n",
    "    return virtual_map\n",
    "\n",
    "virtual_map = create_room_map() \n",
    "# Print map shape\n",
    "print('Map shape:', virtual_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e39c7",
   "metadata": {},
   "source": [
    "The `plot_virtual_map()` function is provided to help you visually interpret the process and understand what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370326b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_virtual_map(virtual_map, dimensions_only = False):\n",
    "    \"\"\"\n",
    "    Plots the robot pose, virtual map, beam endpoints, and optionally grid cells to be updated.\n",
    "    \n",
    "    Parameters:\n",
    "    - virtual_map: environment representation using lines.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the virtual map (environment boundaries or walls)\n",
    "    if not dimensions_only:\n",
    "        plt.plot(virtual_map[0, :], virtual_map[1, :], 'k-')    \n",
    "    \n",
    "    # Set grid and axis limits\n",
    "    plt.grid()\n",
    "    plt.xlim([np.nanmin(virtual_map[0])-2,np.nanmax(virtual_map[0])+2]) # nanmin ignores nan numbers\n",
    "    plt.ylim([np.nanmin(virtual_map[1])-2,np.nanmax(virtual_map[1])+2])  \n",
    "    \n",
    "    # Title and axis labels\n",
    "    plt.title('Living-room Map')\n",
    "    plt.xlabel('X position (m)')\n",
    "    plt.ylabel('Y position (m)')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "plot_virtual_map(virtual_map);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cace41",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Taking an observation</i></b></span>** \n",
    "\n",
    "To use the scanner, we will call the `take_observation()` method, passing it the map as an argument.\n",
    "\n",
    "Similarly to the landmark-based sensors we saw in the last notebook, our laser scanner will produce observations as angle-distance pairs. In other words, the measurements are in *polar coordinates*. Remember that you can express a sensor measurement given in polar coordinates ($z_p=[r,\\alpha]^T$) as cartesian coordinates ($z_c=[z_x,z_y]^T)$ with the following expression:\n",
    " \n",
    " $$\n",
    "     z_c = \\begin{bmatrix} z_x \\\\ z_y \\end{bmatrix} \n",
    "         = \\begin{bmatrix} r \\ cos\\alpha \\\\ r \\ sin\\alpha \\end{bmatrix} \n",
    " $$ \n",
    "\n",
    "Remember also that all the observations are taken from the point of view of the sensor, and thus are expressed in its local system of reference. To express the measurements in world space (on the map), you can use our old trick: **pose composition**.\n",
    "\n",
    "Let's put all this into practice. Implement a function which takes a sensor reading $z$ (which is a **list** of points, in polar coordinates and relative to the sensor pose), and plots those points on the map as a *point cloud*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95925a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observations(z, pose, fig, ax, draw_lines = True, point_color = 'r'):\n",
    "    \"\"\"Draw a sensor observation taken from a given pose on a figure.\n",
    "\n",
    "    Keyword arguments:\n",
    "    z -- Laser observation\n",
    "    pose -- Pose from which the observation was taken\n",
    "    fig, ax -- Figure to plot in\n",
    "    \"\"\"\n",
    "    for i in range (z.shape[1]):\n",
    "        distance = None\n",
    "        angle = None\n",
    "\n",
    "        # calculate the cartesian coordinates of the observed point in local space\n",
    "        x_local = None\n",
    "        y_local = None\n",
    "\n",
    "        # compose with the robot pose to express the point in global space\n",
    "        pose_obs_global = tcomp(None, None)\n",
    "        x = pose_obs_global[0]\n",
    "        y = pose_obs_global[1]\n",
    "\n",
    "        if draw_lines:\n",
    "            ax.plot([x, pose[0]], [y, pose[1]], 'g--', linewidth=1)\n",
    "        ax.plot(None, None, 'o', markersize=4, color=point_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d312a53",
   "metadata": {},
   "source": [
    "Use the following code to test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_laser_and_plot(laser, pose, fig, ax, draw_lines = True, color='r'):\n",
    "    laser.set_pose(pose)\n",
    "\n",
    "    # take the measurement\n",
    "    measurement = laser.take_observation(virtual_map)\n",
    "\n",
    "    # Plot!\n",
    "    DrawRobot(fig, ax, pose, axis_percent=0.015, color=color, linewidth=1.5);\n",
    "    plot_observations(measurement, pose, fig, ax, draw_lines, color)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def test_laser(FOV, resolution, max_distance, distance_noise, angle_noise):\n",
    "    laser = create_laser_at_origin(FOV, resolution, max_distance, np.diag([distance_noise, angle_noise]))\n",
    "\n",
    "    fig, ax = plot_virtual_map(virtual_map);\n",
    "    run_laser_and_plot(laser, np.vstack([6, 4, np.pi/2]), fig, ax);\n",
    "\n",
    "ipywidgets.interact(\n",
    "    test_laser,\n",
    "    FOV=ipywidgets.FloatSlider(3 * np.pi/2, min=0.2, max=2 * np.pi),\n",
    "    resolution=ipywidgets.FloatSlider(0.2, min=0.02, max=0.5, step=0.01),\n",
    "    max_distance=ipywidgets.FloatSlider(10, min=1, max=10),\n",
    "    distance_noise=ipywidgets.FloatSlider(0.01, min=0.0, max=0.2, step=0.01),\n",
    "    angle_noise=ipywidgets.FloatSlider(0.001, min=0, max=0.2, step=0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ba42e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<figure style=\"\">\n",
    "    <img src=\"images/assignment_2_1.png\" width=\"600px\" alt=\"\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a97df",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it</i></b></font>\n",
    "\n",
    "Having completed the code above, you will be able to **answer the following questions**:\n",
    "\n",
    "- Which has a greater impact on the reliability of the measurements: the variance of the distance reading, or the variance of the angle at which the measurement happens? Which would you guess is larger in real sensors, and why?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- Without actually implementing it: how could we calculate the uncertainty about the position of each of the measured points (in cartesian coordinates)? *Hint: we already did this for the landmarks!*\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "- We are assuming we know the pose of the robot, but what if there was also uncertainty associated with it? How do you combine both sources of uncertainty? *Hint: we **also** did this for the landmarks!*\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd119017",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Exploring the environment</i></b></span>** \n",
    "\n",
    "Now that we can plot the measurements in global coordinates, we should be able to roughly reconstruct the contours of the map by moving around the environment and repeatedly taking observations from multiple poses. Let's try that out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17203659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_environment():\n",
    "    laser = create_laser_at_origin(FOV = 3 * np.pi/2,\n",
    "        resolution = 0.05,\n",
    "        max_distance = 10,\n",
    "        noise_cov = np.diag([0.03, 0.01]))\n",
    "\n",
    "    # The robot must make these movements, in this order\n",
    "    # 0, 0, 0\n",
    "    # 2, 0, 0\n",
    "    # 2, 0, 0\n",
    "    # 0, 2, np.pi/2\n",
    "    # 2, 0, 0\n",
    "    # Format them correctly in the following list\n",
    "\n",
    "    # each pose and its corresponding observations will have a different color\n",
    "    pose_increments_list = [\n",
    "        (None, 'orangered'),\n",
    "        (None, 'olive'),\n",
    "        (None, 'teal'),\n",
    "        (None, 'magenta'),\n",
    "        (None, 'indigo'),\n",
    "    ]\n",
    "\n",
    "    fig, ax = plot_virtual_map(virtual_map, True); #create the base figure with the map dimensions, but without the lines\n",
    "\n",
    "    current_pose = np.vstack([4,4,0]) # starting pose\n",
    "\n",
    "    # take a step and read the sensor at the new pose\n",
    "    for step, color in pose_increments_list:\n",
    "        current_pose = None \n",
    "        run_laser_and_plot(None, None, fig, ax, False, color)\n",
    "\n",
    "explore_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e87b5e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<figure style=\"\">\n",
    "    <img src=\"images/assignment_2_2.png\" width=\"600px\" alt=\"\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9017b",
   "metadata": {},
   "source": [
    "This looks pretty promising! Looking at this image, it is quite easy to get an intuition for the shape of the environment and the presence of obstacles. However, due to the noise in the sensor observations, it is difficult to tell where *exactly* the contours of the map really are. \n",
    "\n",
    "This is the problem of **mapping**, which we will talk about in more detail in chapter 6. It probably won't surprise you to learn that, in order to deal with these noisy observations, we will tackle this problem probabilistically. \n",
    "\n",
    "An even more complicated version of this process is necessary when the pose of the robot is uncertain, as we can't even reliably place the noisy points on the global map. To solve this (more realistic) case, we would need to simultaneously figure out the correct pose of the robot *and* build the map. This is the wonderfully named problem of **SLAM:** *Simultaneous Localization And Mapping*, which we will talk about in chapter 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bd4f7",
   "metadata": {},
   "source": [
    "## <font color=\"orange\"><b><i>AI Appendix</i></b></font>\n",
    "\n",
    "### A.1 How I used AI\n",
    "- **Model/tool:**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **What I asked AI to help with (bullets):**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **Best prompt I used (paste):**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **What I kept vs. changed from the AI output:**\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "### A.2 AI review \n",
    "Ask one generative AI to review your memo using this prompt:\n",
    "\n",
    "```\n",
    "Act as a technical reviewer of this computer vision lab notebook. Read my memo and return exactly 5 bullet points:\n",
    "\n",
    "- two on theory (key ideas),\n",
    "- two on practice (relevant programming concepts of python used, not particular functions or variables),\n",
    "- one on how to improve the code (most critical aspect: limitations, parameter choices, runtime/robustness, possible bugs).\n",
    "\n",
    "Keep each bullet â‰¤15 words and make them specific to my work.\n",
    "```\n",
    "\n",
    "Paste its **5** items here:\n",
    "\n",
    "- **(Theory 1)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Theory 2)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Practice 1)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Practice 2)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "- **(Improvement)** AI claim:\n",
    "  <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "\n",
    "Finally, **do you agree with them?**, **would you have selected others?** \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
